{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 : Recurrent Neural Networks \n",
    "\n",
    "## Recurrent Neural Networks \n",
    "\n",
    "### Why Sequence models \n",
    "\n",
    "Examples of sequence data:\n",
    "* speech recognition \n",
    "* music generation \n",
    "* sentiment classification \n",
    "* DNA sequence analysis\n",
    "* machine translation \n",
    "* video activity recognition \n",
    "* name entity recognition\n",
    "\n",
    "### Notation \n",
    "\n",
    "$x$ : Harry Potter and Hermione Granger invented a new spell. \n",
    "\n",
    "$x$ : $x^{<1>} = \\text{Harry}, x^{<2>} = \\text{Potter}, \\ldots, x^{<9>} = \\text{spell}$ \n",
    "\n",
    "$y$ : $y^{<1>} = 1, y^{<2>} = 1, \\ldots, y^{<9>} = 0$\n",
    "\n",
    "$x^{(i)<t>}$ : represents the $i$th training example and the $t$th object in the sequence\n",
    "\n",
    "$T_x, T_y$ : the length of the input sequence and the length of the output sequence, respectively. Varies, obviously, for each training example\n",
    "\n",
    "We store all the words in a dictionary/vocabulary\n",
    "\n",
    "We do one-hot encoding for each object in our input sequence for each input sequence in our training set such that: \n",
    "$$x^{(i)<1>} = \\text{Harry} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\  0 \\end{bmatrix}$$\n",
    "\n",
    "In dealing with words not in the vocabulary:\n",
    "$$<\\text{unk}> = \\text{some_index (maybe the last) in vocab/dict}$$\n",
    "\n",
    "### Recurrent Neural Network Model \n",
    "\n",
    "Why not a standard FFNN? \n",
    "* different inputs/outputs in the training set maybe have different lengths\n",
    "* doesn't share features learned across different positions of a text (no knowledge of sequences) \n",
    "\n",
    "**Recurrent Neural Network**\n",
    "\n",
    "Similar explanation to what you saw in udacity course \n",
    "\n",
    "* activation $a^{<t>}$ is saved across multiple time steps  \n",
    "* parameters are shared between time steps \n",
    "\n",
    "Weakness in the current RNN example: \n",
    "* only glean information from the previous parts of the sequence, not from later \n",
    "    * He said, \"Teddy Roosevelt was a great President.\" \n",
    "    * He said, \"Teddy bears are on sale!\" \n",
    "\n",
    "    * BRNN (Bi-directional RNN) helps this \n",
    "    \n",
    "RNN calculations:\n",
    "* for some timestep $t$ in this (weak) RNN\n",
    "    * $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a$) where $g$ is maybe $\\text{tanh}$ or $\\text{ReLU}$\n",
    "    * $\\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y)$\n",
    "\n",
    "* simplify that notation above ^\n",
    "   * $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$ is simplified to $ g(W_{a}[a^{<t-1>},x^{<t>}] + b_a)$\n",
    "       * $W_{aa}$ and $W_{ax}$ are stacked horizontally s.t. $\\begin{bmatrix} W_{aa} & W_{ax} \\end{bmatrix} = W_{a}$\n",
    "       * $[a^{<t-1>},x^{<t>}] = \\begin{bmatrix} a^{<t-1>} \\\\ x^{<t>} \\end{bmatrix}$\n",
    "       \n",
    "   * what this does is compresses the number of parameter matrices from 2 to 1 \n",
    "   \n",
    "   * similarly, for $\\hat{y}$, we have $\\hat{y}^{<t>} = g(W_{y}a^{<t>} + b_y)$ (aka we excluded the subscript $a$)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time \n",
    "\n",
    "Define the loss function:\n",
    "$\\mathcal{L}^{<t>}(x^{<t>},y^{<t>}) = -y^{<t>}\\log{\\hat{y}^{<t>}} - (1-y^{<t>})\\log{(1-\\hat{y}^{<t>})} $ \n",
    "\n",
    "$\\mathcal{L}(\\hat{y},y) = \\sum\\limits_{t=1}^{T_y}\\mathcal{L}^{<t>}(\\hat{y}^{<t>},y^{<t>})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of RNNs\n",
    "\n",
    "Many to many where the input and output lengths are different \n",
    "* Machine (spoken) language translation \n",
    "    * encoder and decoder \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanuage Model and Sequence Generation \n",
    "\n",
    "What is language modelling?\n",
    "Gives out probabilities based on how likely a sentence is actually phrased\n",
    "\n",
    "Speech Recognition\n",
    "* $P($The apple and pair salad$)$ \n",
    "* $P($The apple and pear salad$)$\n",
    "* $P(\\text{sentence_1}) = ?$\n",
    "    * if you were to pick up a random newspaper/email/next thing your friend is saying, what is the chance that the next sentence you use will be $\\text{sentence_1}$ ? \n",
    "    * $P(y^{<1>}, y^{<2>}, \\ldots, y^{<n>})$ : What is the probability of that particular sequence of words\n",
    "\n",
    "#### How do you build a language model?? \n",
    "\n",
    "Training set : a large corpus of text \n",
    "\n",
    "**Tokenize** : form a dictionary for each object, in this case, word. Pair them with one-hot vectors \n",
    "* some models have <EOS> token, but we won't use that here \n",
    "* <UNK> will be used tho, to denote words that are not in our vocabulary/dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ng goes through a language model RNN example \n",
    "* given the first $t$ words, what is the $P$ or probability or chance of ALL the words (the most likely having the highest probability) (softmax is so rigged for this)   \n",
    "    * 'next word' predictor \n",
    "    * multiply out the probabilities  \n",
    "    \n",
    "loss function would be: $\\mathcal{L} = \\sum\\limits_t \\mathcal{L}(\\hat{y}^{<t>}, y^{<t>})$ , pretty analogous to what we've seen above. really cool! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Novel Sequences \n",
    "\n",
    "'Word level RNN':\n",
    "\n",
    "For the first timestep in a sequence, randomly sample a softmax distribution. For each subsequent word in the sequence, what is the probability of the next word given all the previous words? \n",
    "\n",
    "Maybe you can do 'character level RNN', where you go through the timesteps by characters instead of words, so that you can avoid getting $<\\text{UNK}>$s, but the main disadvantage is you end up with much longer sequences, (aka computationally expensive!)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient with RNNs\n",
    "\n",
    "Hard to capture long-term dependencies with traditional RNNs\n",
    "* The **cat**, which already ate ..., **was** full.\n",
    "* The **cats**, which already ate ..., **were** full.\n",
    "\n",
    "Errors vanish for timesteps that were way farther back in the neural network.\n",
    "\n",
    "For the exploding gradient problem, we address that through **gradient clipping**.\n",
    "    * gradient clipping: after some threshold, the rescale the gradient vectors that are past the threshold. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)\n",
    "\n",
    "Handles the vanishing gradient problem well.\n",
    "\n",
    "SUPER RECENT papers: \n",
    "* Cho et al., 2014. On the properties of neural machine translation: Encoder-decoder approaches\n",
    "* Chung et al., 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling \n",
    "\n",
    "**More notation**\n",
    "\n",
    "$c$ = memory cell  \n",
    "* for this current basic RNN, we have $c^{<t>} = a^{<t>}$, but this is not the case (such as for LSTMs)\n",
    "\n",
    "$\\tilde{c}^{<t>} = \\tanh(W_{c}[c^{<t-1>},x^{<t>}] + b_c)$\n",
    "* $\\tilde{c}^{<t>}$ is a candidate for replacing $c^{<t>}$ \n",
    "\n",
    "$\\Gamma_u$ the gate. An update gate s.t. it's values are between $0$ and $1$.\n",
    "* In practice, it's applied as $\\Gamma_u = \\sigma(W_{u}[c^{<t-1>},x^{<t>}] + b_u)$\n",
    "    * it's values are very close to 0 or 1 most of the time\n",
    "* Even though we ultimately evaluate with $\\sigma$, imagine that this update gate is $0$ or $1$ most of the time \n",
    "\n",
    "Then, we have a generalization of how to form a memory cell $c$ at timestep $t$:\n",
    "* $c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}$\n",
    "    * that is, either keep the candidate of the cell or keep the old cell value \n",
    "        * boom, easy \n",
    "        \n",
    "How to implement?\n",
    "* $c^{<t>}, \\tilde{c}^{<t>},$ and $\\Gamma_u$ are all the same dimensions\n",
    "\n",
    "$\\Gamma_r$ : relevance gate. How relevant is the previous previous word to the word candidate. Applied against the candidate gate $\\tilde{c}^{<t>}$ s.t. : \n",
    "* $\\Gamma_r = \\sigma(W_{r}[c^{<t-1>},x^{<t>}] + b_r)$\n",
    "* $\\tilde{c}^{<t>} = \\tanh(W_{c}[\\Gamma_r * c^{<t-1>},x^{<t>}] + b_c)$\n",
    "\n",
    "Thus, the entire GRU process goes like this for each training example: \n",
    "* $\\Gamma_r = \\sigma(W_{r}[c^{<t-1>},x^{<t>}] + b_r)$\n",
    "* $\\tilde{c}^{<t>} = \\tanh(W_{c}[\\Gamma_r * c^{<t-1>},x^{<t>}] + b_c)$\n",
    "* $\\Gamma_u = \\sigma(W_{u}[c^{<t-1>},x^{<t>}] + b_u)$\n",
    "* $c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}$\n",
    "* $a^{<t>} = c^{<t>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM) unit \n",
    "\n",
    "OLD paper but still relevant! \n",
    "* Hochreiter & Schmidhuber 1997. Long short-term memory   \n",
    "\n",
    "The presented LSTM process looks like this: \n",
    "* $\\tilde{c}^{<t>} = \\tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c)$\n",
    "* $\\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u)$ update gate\n",
    "* $\\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f)$ forget gate \n",
    "* $\\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o)$ output gate \n",
    "* $c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + \\Gamma_f * c^{<t-1>}$  \n",
    "* $a^{<t>} = \\Gamma_o * \\tanh(c^{<t>})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN (BRNN)\n",
    "\n",
    "Take information from both earlier and later in the sequence \n",
    "\n",
    "Take a second cell per timestep and basically start the forward prop from right to left instead of left to right.\n",
    "* Each cell has 2 activations, one for left to right and another from right to left \n",
    "    * use both cells to make a prediction per time step \n",
    "\n",
    "* $\\hat{y}^{<t>} = g(W_y[\\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>}] + b_y)$\n",
    "\n",
    "BRNN w/ LSTM blocks are pretty reasonable first thing to try "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNNs\n",
    "\n",
    "Don't see really deep RNNs because of the unfolding through time \n",
    "* later layers may not have timesteps in them, more vanilla NNs to deal with after a point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignments\n",
    "\n",
    "### Buildling a Recurrent Neural Network \n",
    "\n",
    "What we did: \n",
    "* implemented a basic (unfolded) RNN cell using numpy\n",
    "    * used the following equations for this: \n",
    "    * $a^{<t>} = \\tanh(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a$)\n",
    "    * $\\hat{y}^{<t>} = \\text{softmax}(W_{ya}a^{<t>} + b_y)$\n",
    "* implemented RNN forward prop\n",
    "* implemented an LSTM cell using numpy\n",
    "    * used the following equations for this:\n",
    "    * $\\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}] + b_u)$ update gate\n",
    "    * $\\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}] + b_f)$ forget gate \n",
    "    * $\\tilde{c}^{<t>} = \\tanh(W_c[a^{<t-1>}, x^{<t>}] + b_c)$\n",
    "    * $c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + \\Gamma_f * c^{<t-1>}$  \n",
    "    * $\\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}] + b_o)$ output gate \n",
    "    * $a^{<t>} = \\Gamma_o * \\tanh(c^{<t>})$\n",
    "* implemented feed forward LSTM network of one layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**:\n",
    "1. Concatenate $a^{\\langle t-1 \\rangle}$ and $x^{\\langle t \\rangle}$ in a single matrix: $concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$\n",
    "2. Compute all the formulas 1-6. You can use `sigmoid()` (provided) and `np.tanh()`.\n",
    "3. Compute the prediction $y^{\\langle t \\rangle}$. You can use `softmax()` (provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUST COMPLETE THE BACKPROP IMPLEMENTATION OF LSTM NETWORKS PROG ASSIGNMENT1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character level language model \n",
    "\n",
    "We implemented using only numpy:\n",
    "* RNN character level language model with:\n",
    "    * gradient clipping (avoid that there exploding gradient)\n",
    "        * np.clip(..., out=) IS REALLY FANCY WOW\n",
    "    * sampling (to generate characters) using the following equations:\n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)$$\n",
    "\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y $$\n",
    "\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })$$\n",
    "\n",
    "Got stuck with np zero dimensional arrays again.    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization algorithm used for this character level language sequence model \n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python quirks:\n",
    "```    \n",
    "           x = Lambda(lambda x: X[:,t,:])(X)\n",
    "``` \n",
    "signifies that the Lambda function is returning some function that is callable. Just doing the call in-line.\n",
    "\n",
    "Again, need to understand Keras/Python/errthang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvise a Jazz Solo with an LSTM Network\n",
    "\n",
    "Generated a jazz solo using keras and a basic LSTM arch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 \n",
    "\n",
    "# Introduction to Word Embeddings \n",
    "\n",
    "### Word Representation \n",
    "\n",
    "One hot representation weakness \n",
    "* Weakness of having one-hot encoding is that every one-hot vector in the one-hot encoding matrix is an object in itself.\n",
    "    * Man, Woman, King, Queen, Apple, and Orange are all their own one-hot vector an there is no relationship between like terms. The rep will treat the relationship between apple and orange the same way as apple and man\n",
    "        * This is because the inner product of any two one-hot vectors is $0$, so it won't know!\n",
    "        \n",
    "Featurized representation to fix this maybe?\n",
    "* embeddings will be used as feature representations of one-hot vectors!\n",
    "    * example: a vector of 300 features values will represent a male, and another 300 feature vector with different values will represent a female\n",
    "* each of these feature vectors give a better representation than repping the objects as just one-hot vectors\n",
    "\n",
    "* t-SNE representation! represent high dimensional feature vectors (300 in the example) as a 2 dim graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
