{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 : Recurrent Neural Networks \n",
    "\n",
    "## Recurrent Neural Networks \n",
    "\n",
    "### Why Sequence models \n",
    "\n",
    "Examples of sequence data:\n",
    "* speech recognition \n",
    "* music generation \n",
    "* sentiment classification \n",
    "* DNA sequence analysis\n",
    "* machine translation \n",
    "* video activity recognition \n",
    "* name entity recognition\n",
    "\n",
    "### Notation \n",
    "\n",
    "$x$ : Harry Potter and Hermione Granger invented a new spell. \n",
    "\n",
    "$x$ : $x^{<1>} = \\text{Harry}, x^{<2>} = \\text{Potter}, \\ldots, x^{<9>} = \\text{spell}$ \n",
    "\n",
    "$y$ : $y^{<1>} = 1, y^{<2>} = 1, \\ldots, y^{<9>} = 0$\n",
    "\n",
    "$x^{(i)<t>}$ : represents the $i$th training example and the $t$th object in the sequence\n",
    "\n",
    "$T_x, T_y$ : the length of the input sequence and the length of the output sequence, respectively. Varies, obviously, for each training example\n",
    "\n",
    "We store all the words in a dictionary/vocabulary\n",
    "\n",
    "We do one-hot encoding for each object in our input sequence for each input sequence in our training set such that: \n",
    "$$x^{(i)<1>} = \\text{Harry} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\  0 \\end{bmatrix}$$\n",
    "\n",
    "In dealing with words not in the vocabulary:\n",
    "$$<\\text{unk}> = \\text{some_index (maybe the last) in vocab/dict}$$\n",
    "\n",
    "### Recurrent Neural Network Model \n",
    "\n",
    "Why not a standard FFNN? \n",
    "* different inputs/outputs in the training set maybe have different lengths\n",
    "* doesn't share features learned across different positions of a text (no knowledge of sequences) \n",
    "\n",
    "**Recurrent Neural Network**\n",
    "\n",
    "Similar explanation to what you saw in udacity course \n",
    "\n",
    "* activation $a^{<t>}$ is saved across multiple time steps  \n",
    "* parameters are shared between time steps \n",
    "\n",
    "Weakness in the current RNN example: \n",
    "* only glean information from the previous parts of the sequence, not from later \n",
    "    * He said, \"Teddy Roosevelt was a great President.\" \n",
    "    * He said, \"Teddy bears are on sale!\" \n",
    "\n",
    "    * BRNN (Bi-directional RNN) helps this \n",
    "    \n",
    "RNN calculations:\n",
    "* for some timestep $t$ in this (weak) RNN\n",
    "    * $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a$) where $g$ is maybe $\\text{tanh}$ or $\\text{ReLU}$\n",
    "    * $\\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y)$\n",
    "\n",
    "* simplify that notation above ^\n",
    "   * $a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$ is simplified to $ g(W_{a}[a^{<t-1>},x^{<t>}] + b_a)$\n",
    "       * $W_{aa}$ and $W_{ax}$ are stacked horizontally s.t. $\\begin{bmatrix} W_{aa} & W_{ax} \\end{bmatrix} = W_{a}$\n",
    "       * $[a^{<t-1>},x^{<t>}] = \\begin{bmatrix} a^{<t-1>} \\\\ x^{<t>} \\end{bmatrix}$\n",
    "       \n",
    "   * what this does is compresses the number of parameter matrices from 2 to 1 \n",
    "   \n",
    "   * similarly, for $\\hat{y}$, we have $\\hat{y}^{<t>} = g(W_{y}a^{<t>} + b_y)$ (aka we excluded the subscript $a$)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time \n",
    "\n",
    "Define the loss function:\n",
    "$\\mathcal{L}^{<t>}(x^{<t>},y^{<t>}) = -y^{<t>}\\log{\\hat{y}^{<t>}} - (1-y^{<t>})\\log{(1-\\hat{y}^{<t>})} $ \n",
    "\n",
    "$\\mathcal{L}(\\hat{y},y) = \\sum\\limits_{t=1}^{T_y}\\mathcal{L}^{<t>}(\\hat{y}^{<t>},y^{<t>})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of RNNs\n",
    "\n",
    "Many to many where the input and output lengths are different \n",
    "* Machine (spoken) language translation \n",
    "    * encoder and decoder \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanuage Model and Sequence Generation \n",
    "\n",
    "What is language modelling?\n",
    "Gives out probabilities based on how likely a sentence is actually phrased\n",
    "\n",
    "Speech Recognition\n",
    "* $P($The apple and pair salad$)$ \n",
    "* $P($The apple and pear salad$)$\n",
    "* $P(\\text{sentence}) = ?$\n",
    "\n",
    "#### How do you build a language model?? \n",
    "\n",
    "Training set : a large corpus of text \n",
    "\n",
    "**Tokenize** : form a dictionary for each object, in this case, word. Pair them with one-hot vectors \n",
    "* some models have <EOS> token, but we won't use that here \n",
    "* <UNK> will be used tho, to denote words that are not in our vocabulary/dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ng goes through a one to many RNN example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
