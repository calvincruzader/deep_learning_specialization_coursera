{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "course4_week2_notes: Deep Convolutional Models by Case Studies \n",
    "--------------\n",
    "Case studies:\n",
    "--------------\n",
    "\n",
    "**Why look at case studies?**\n",
    "* to gain intuition of how to build NNs\n",
    "* plus, many convNet architectures can be transferred from one problem to another \n",
    "\n",
    "**Classic Networks** \n",
    "* LeNet-5: LeCun et al., 1998. Gradient-based learning applied to document recognition \n",
    "  * Goal: recoognize handwritten digits \n",
    "  * patterns that we still see today:\n",
    "    * $n_H$, $n_{W}$ goes down while $n_C$ goes up as we go down the layers \n",
    "![LeNet](https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/img/lenet-result.png)\n",
    "* AlexNet: Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks \n",
    "  * similar to LeNet but much bigger, 60million parameters \n",
    "    * demonstrates scaling of a robust architecture despite much more data \n",
    "  * used ReLU\n",
    "  * Local response normalization (LRN) (?)\n",
    "![AlexNet_pic](https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/Model%20&%20ImgNet/alexnet/img/alexnet2.png)\n",
    "* VGG-16: Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition \n",
    "  * simplified neural network architectures \n",
    "  * all convolution layers had 3x3 filters, a stride of 1, 'same' convolution. All max_pool layers were 2x2 with a stride of 2         \n",
    "    * number of filters for convolutions doubles from initial 64 until 512\n",
    "  ![VGG_pic](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)\n",
    "\n",
    "**ResNets**\n",
    "* vanishing and exploding gradients is a big problem, resnets take care of that \n",
    "* Residual Network\n",
    "* Residual block \n",
    "  * $a^{[l+2]} = g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]})$\n",
    "  * if you're using $L_2$ regularization, $W^{[l+2]}$ will tend to shrink and if $W^{[l+2]}=0 \\Rightarrow g(a^{[l]}) = a^{[l]}$\n",
    "    * $\\therefore$ the identity function is easy for the residual block to learn!\n",
    "      * Also, this means you need both $a^{[l]}, a^{[l+2]} \\in \\Re^{m \\times n}$ to add the residual block, so 'same' convolutions are used during these 'skip connection' steps \n",
    "        * if the dimensions don't match, then an option would be : $g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + W_{s}a^{[l]})$\n",
    "  * Res blocks allows you to train **much** deeper neural nets\n",
    "  * Training error keeps on going down as the number of layers increases, whereas in plain networks, the training error goes up after a certain point when the neural net has 'too many' layers \n",
    "    * takes care of the vanishing and exploding gradient problem \n",
    "  * Informal thoughts : skipping neurons skip any vanishing or exploding functionality that might happen in the very next layer, so that, just in case the neurons DO vanish/explode with that next function, there'll be a fallback of the next non-linearity that does not get impacted by vanishing/exploding gradients \n",
    "\n",
    "**Why do ResNets work?**\n",
    "* ResNets provide a baseline such that the activation of the later layers cannot get worse, only as bad as that previous layer \n",
    "  * makes training error pretty much a non-increasing function of the number of layers in a NN \n",
    "    * doesn't hurt performance \n",
    "* paper: He et al., 2015. Deep Residual Networks for Image Recognition \n",
    "\n",
    "**1x1 Convolutions aka Networks in Networks**\n",
    "* Lin et al., 2013, Network in Network\n",
    "* focuses on the **number of channels** in a current layer so that you can mutate it (increase/decrease/mess with complexity) in the next layer \n",
    "* As mentioned, you can focus on getting fully connected networks on channels and get a nonlinearity to get a more complex function \n",
    "* In my own words:\n",
    "    * doing 1x1 convolutions makes sense when you want to focus on the number of channels of a given layer. you can shrink/increase/maintain the size of the input channels by giving a smaller/larger/same size number of filters. \n",
    "    * doing 1x1 convolutions also makes sense when you want to feed the input into a nonlinear activation function by channel (giving a fc layer such that (number input channels convolvedBy number of filters -> number output channels) $n_{c_i} \\circledast n_{f} = n_{c_i}$ Given $X$ has dimensions $(n_{h_i}, n_{w_i}, n_{c_i})$ and there is a filter $F$ such that $F$has dimensions $(f , f, n_f)$ where $f \\in \\mathcal{R}^1, f < n_{h_i}, f < n_{w_i}$ then $X \\circledast F = Z$ where $Z$ has dimensions $(n_{h_o}, n_{w_o}, n_{c_o})$ and $n_{c_o} = n_{f}$.\n",
    "    \n",
    "\n",
    "\n",
    "**Inception Network Motivation**\n",
    "* Szegedy et al., 2014. Going deeper with vonvolutions \n",
    "* underlying motivation: How does one choose which filter/kernel to use for a ConvNN? \n",
    "  * you can choose by doing an inception layer! perform a 1x1, 3x3, 5x5, maxpool all in one layer and stack the outputs up\n",
    "  * let the NN learn whatever parameters it wants to use or whatever convolution filter sizes it wants \n",
    "    * the problem here is computational cost \n",
    "    * you can use 1x1 convolution to reduce the number of channels of a lyer to reduce the computational cost by (example used caused a reduction by an order of magnitude)\n",
    "      * does shrinking down the representation size (many channels to much fewer channels) hurt performance? \n",
    "        * so long as you shrink down 'within reason' it doesn't hurt performance and saves a lot of computation \n",
    "\n",
    "**Inception Network** \n",
    "* Inception network is just an inception model repeated a bunch of times into a network \n",
    "<img src=\"http://img.blog.csdn.net/20160225155403967\" style=\"width: 800px; height=320px\"> \n",
    "\n",
    "--------------\n",
    "Practical Advice for Using ConvNets\n",
    "--------------\n",
    "**Using Open-Source Implementation**\n",
    "* advantages: \n",
    "  * don't have to write up your own NNs\n",
    "  * parameters in these open-source sources can be pretrained\n",
    "\n",
    "**Transfer Learning**\n",
    "* for your small training sets:\n",
    "  * freeze parameters and train only your own custom softmax layer at the very end \n",
    "* for your larger training sets \n",
    "  * freeze less layers \n",
    "* for your very large data sets:\n",
    "  * use the whole network as **initialization** , replaces random initialization this way \n",
    "\n",
    "**Data Augmentation**\n",
    "* common to increase performance of a given system\n",
    "  * in the field of computer vision, we can't get enough data!\n",
    "* common data augmentation methods:\n",
    "  * mirroring, random cropping, rotation, shearing, local warping \n",
    "  * color shifting: RGB (+20,20,+20) or (-20,+20,+20)\n",
    "    * these RGB values are drawn from some probability distribution \n",
    "    * principles components analysis: PCA color augmentation\n",
    "      * keep the overall color tint the same \n",
    "      * details in AlexNet \n",
    "    * implementing distortions \n",
    "      * have a CPU thread for doing distortions on mini batches \n",
    "      * have another CPU/GPU thread on training data \n",
    "        * can do both above in parallel \n",
    "\n",
    "**The State of Computer Vision** \n",
    "* data vs. hand-engineering \n",
    "  * we still need to have more data for image recognition \n",
    "* if you only have a little data, there's more hand-engineering going on \n",
    "* For deep learning, learning algorithms have two sources of knowledge:\n",
    "  1. Labeled data\n",
    "  2. Hand engineering (features/network architecture/other components/transfer learning) \n",
    "     * in the absense of lots of data, hand engineering works very well \n",
    "\n",
    "* Tips for doing well on benchmarks/winning competitions  \n",
    "  * ensembling: train several NNs (3-15 networks) independently and average their outputs (will perform 1% or 2% better)\n",
    "  * multi-crop at test time: run a classifier on multiple versions of test images and average the results \n",
    "    * 10 - crop: center, zoomed a little in each corner, then flip , then zoom again the 4 corners  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
