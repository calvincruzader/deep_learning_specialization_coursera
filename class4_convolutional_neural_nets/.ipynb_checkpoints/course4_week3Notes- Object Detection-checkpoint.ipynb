{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "course4_week3notes: Object Detection \n",
    "\n",
    "-------\n",
    "Detection Algorithms\n",
    "-------\n",
    "**Object Localization**\n",
    "* Recall that building a ConvNet results in a set of features being fed into a softmax unit that outputs a predicted class ie a classification output. What if you wanted to localize the classification detected, then you would add localization numbers $(b_x, b_y, b_h, b_w)$ standing for $\\text{(x_position, y_position, height, width)}$ to parameterize the **bounding box** of the detected object. \n",
    "\n",
    "* Thus, the target label $y$ is defined/modifed as follows: $$y= \\left( \\begin{matrix}{} P_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ c_1 \\\\ c_2 \\\\ \\vdots \\end{matrix} \\right)$$ \n",
    "where \n",
    "* $P_c$ denotes the probability that any object is in the image \n",
    "* $b_x, b_y, b_h, b_w$ are the localization numbers that form the bounding box \n",
    "* $c_1, c_2,...$ are the set of objects you want to classify.\n",
    "  * Note: This assumes that there is only one object to classify in the image \n",
    "  * Examples: Given that $c_1$ will denote the probability of a pedestrian is in the image, $c_2$ a car, $c_3$ a motorcycle, and $c_4$ a background (no object detected), then:\n",
    "    <div>$x_1=$ <img src=\"http://www.publicdomainpictures.net/pictures/30000/velka/traveling-by-car.jpg\" style=\"width:200px;height:133px\">  $\\Rightarrow$ $y_1=\\left( \\begin{matrix}{} 1 \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\\\ 0 \\\\ 1 \\\\ 0  \\end{matrix} \\right)$,  $x_2=$ <img src=\"https://www.goodfreephotos.com/albums/other-landscapes/landscapes-with-lake-and-clouds.jpg\" alt=\"landscape_pic\" style=\"width:200px;height:133px\"> $\\Rightarrow$ $y_2=\\left( \\begin{matrix}{} 0 \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ? \\\\ ?  \\end{matrix} \\right)$</div>\n",
    "* The **loss function** can then be defined as: $\\mathcal{L}(\\hat{y},y)=\\begin{cases} (\\hat{y}_1-y_1)^2 + (\\hat{y}_2-y_2)^2 + ... + (\\hat{y}_n-y_n)^2 \\kern{1ex} \\text{  if  }   y_1 = 1 \\\\  (\\hat{y}_1-y_1)^2 \\kern{27ex} \\text{  if  } y_1 = 0 \\end{cases}$  \n",
    "  * The loss function above is only an example. You can use different loss functions per section of the $\\hat{y}$, that is: \n",
    "    * Log-likelihood for $c_1,...,c_n$ through softmax\n",
    "    * Squared error for bounding boxes\n",
    "    * Logistic regression loss for the $P_c$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Landmark Detection**\n",
    "* You can also just generalize object localization above to output $(x,y)$ coordinates of important points in an image, called landmarks, that you want the neural network to recognize. \n",
    "  * Let's call each landmark $l_i$ and, for an example let's have a face detector with 64 landmarks for different sections of a person's face.\n",
    "    * ConvNet $\\Rightarrow \\left( \\begin{matrix}{} P_{\\text{face}} \\\\ l_{1x} \\\\ l_{1y} \\\\ \\vdots \\\\ l_{64x} \\\\ l_{64y}  \\end{matrix} \\right)$ \n",
    "  * To train a network like this, you'll need a labeled training set of both x and y (uh oh) \n",
    "  * You could also have a pose detection, such as instead of just face features, entire body features, like the left elbow,  right knew, etc., and generate landmarks based off a perceived person in an image's pose.\n",
    "\n",
    "**Object Detection** \n",
    "* closely cropped images in training set -> convnet -> y \n",
    "  * then use that convnet in a sliding windows detection algorithm. The algorithm is as follows:\n",
    "    * little window on a subsection of the image -> ConvNet -> {0,1}\n",
    "    * shift window based in some stride and iterate throughout the image \n",
    "    * increase the size of the window then repeat steps above steps \n",
    "    * keep increasing the size of the window \n",
    "    * classify each region contained in each window as having a face or whatever or not   \n",
    "    * <img src=\"https://i.vimeocdn.com/filter/overlay?src0=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F72275532_1280x1220.jpg&src1=https%3A%2F%2Ff.vimeocdn.com%2Fimages_v6%2Fshare%2Fplay_icon_overlay.png\" style=\"width: 200px; height:200px\">\n",
    "    * huge **disadvantage** is computational cost: you're using a ConvNet for each and every one of the iterations!\n",
    "      * large strides would hurt performance, small strides would be computationally intensive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Convolutional Implementation of Sliding Windows**\n",
    "* Basically, compute all window predictions at once! \n",
    "    * Before, we had a subsection an image, a window, and we performed our ConvNet on each one of these windows \n",
    "    * Now, we run our ConvNet on all windows at once using a filter/kernel as our window. \n",
    "        * Our filter and strides become our window, and we run the rest of the ConvNet as before.\n",
    "* turn FC layers into convolutional layers (1x1xNum_Filter volumes) by doing convolutions on the layers that were supposed to be FC \n",
    "* Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks \n",
    "* alot of the computation is duplicated, conv impl allows forward passes to share computation \n",
    "* after the last layer, you'll have a **volume** representing the probabilities of existing classes and the bounding box estimations  \n",
    "* still, one disadvantage is that the bounding box is not going to be perfectly centered \n",
    "\n",
    "**Bounding Box Predictions** \n",
    "* Improve bounding box estimation  \n",
    "* Redmon et al., 2015, You Only Look Once: Unified real-time object detection\n",
    "    * hard paper to read >.>\n",
    "* YOLO algorithm, works for real-time object detection!\n",
    "    * YOLO algorithm:  divide an image up into cells (popular right now is 19 x 19) and apply the image classification and localization algorithm on each of these cells \n",
    "        * for each cell, create a $y = (p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3)$\n",
    "        * target output size : $3 \\times 3 \\times (5 +\\text{ num_classifiers})$\n",
    "    * the way you assign an object to a grid cell: whatever grid cell the midpoint of that object is, you assign that object to that grid cell  \n",
    "    * how exactly do you encode the $ b_x, b_y, b_h, b_w$ in the vector $y$?\n",
    "        * treat upper-left corner of a grid cell as $(0,0)$ and bottom-right corner of a grid cell as $(1,1)$\n",
    "        * $b_x, b_y$ are the coordinates of the midpoint so they'll be represented as come fraction s.t. $b_x, b_y \\in [0,1]$\n",
    "        * $b_h, b_w$ could be $> 1$, the image can span across multiple grid cells \n",
    "        \n",
    "**Intersection Over Union**\n",
    "* How do you tell if your object detection algorithm is going well?\n",
    "  * compute the Intersection over Union of ground truth bounding box and hypothesis bounding box \n",
    "    * a hypothesis bounding box is good if IoU $\\geq$ 0.5 (or some other arbitrary hand-picked number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Non-Max Suppression**\n",
    "* there is an issue with YOLO algorithm s.t. there may be multiple object detections in different grid cells for the same object, non-max suppression deals with this issue \n",
    "  * show bounding boxes of predictions with high $P_c$, high chance of having a class detected \n",
    "  * If there are 2 or more bounding boxes with high overlap, then highlight the highest $P_c$ among them and suppress the others  \n",
    "  * non-max suppression algorithm (for one class):\n",
    "    * Each output prediction is: $\\hat{y} = \\left( \\begin{matrix}{} p_c \\\\ b_x \\\\ b_y \\\\ b_h \\\\ b_w \\end{matrix} \\right)$\n",
    "    * Discard all boxes with $p_c \\leq 0.6$ where $0.6$ is hand-picked \n",
    "    * While there are any remaining boxes :\n",
    "      * Pick the box with the largest $p_c$ and output that prediction (aka highlight it)\n",
    "      * Discard any remaining box with IoU $\\geq 0.5$ (hand-picked) with the box output from the previous step\n",
    "  * non-max suppression algorithm (for multiple classes):\n",
    "    * Just perform non-max suppression for each class\n",
    "\n",
    "**Anchor Boxes**\n",
    "* Each of the grid cells for object detection can only detect one object, what if a grid cell detects multiple objects? \n",
    "* anchor boxes : predefined shapes with which you can associate predictions $\\hat{y}$ \n",
    "  * concatenate each anchor parameters into your prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
