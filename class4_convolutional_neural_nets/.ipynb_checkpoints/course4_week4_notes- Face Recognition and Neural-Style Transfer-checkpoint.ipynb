{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition \n",
    "\n",
    "#### What is face recognition? \n",
    "\n",
    "* Face verification vs. face recognition \n",
    "   * Verification - given an image and name/ID, output whether the image is the claimed person \n",
    "   * Recognition - given you have $K$ persons in a database and their respective names/IDs, when you get an input image, output ID if image is any of the $K$ persons (or \"not recognized\")\n",
    "   \n",
    "#### One Shot Learning \n",
    "\n",
    "* One-shot learning - recognize that person given just one image of that person's face\n",
    "    * aka learn from just **one** example \n",
    "    * to make it work, we learn a \"similarity\" function \n",
    "        * $d($img1, img2$)$ $=$ degree of difference between images\n",
    "            * If $d($img1,img2$) \\leq \\tau \\rightarrow$ \"same\" else \"different\"\n",
    "            * pairwise comparison between the input image and each of the $K$ persons in the database\n",
    "            \n",
    "#### Siamese Network \n",
    "\n",
    "* Taigman et. al., 2014. DeepFace closing the gap to human level performance\n",
    "* Siamese Network - running two **identical CNNs** on two different inputs \n",
    "* Siamese Network explained: Suppose you have images $x^{(1)}$ and $x^{(2)} $ such that\n",
    "    * $x^{(1)} \\rightarrow$ ConvNet until last FC layer $\\rightarrow f(x^{(1)}) =$ \"encoding of $x^{(1)}$\"\n",
    "    * $x^{(2)} \\rightarrow$ ConvNet until last FC layer $\\rightarrow f(x^{(2)}) =$ \"encoding of $x^{(2)}$\"\n",
    "    * Then, we can define $d(x^{(1)}, x^{(2)}) = \\Vert {f(x^{(1)}) -  f(x^{(2)})}\\Vert^2_2$ such that:\n",
    "        * $\\Vert {f(x^{(i)}) -  f(x^{(j)})}\\Vert^2_2$ is small if $x^{(i)}$ and  $x^{(j)}$ are the same person \n",
    "        * $\\Vert {f(x^{(i)}) -  f(x^{(j)})}\\Vert^2_2$ is large if $x^{(i)}$ and  $x^{(j)}$ are different people \n",
    "            * Use backprop until all these conditions are satisfied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            \n",
    "#### Triplet Loss \n",
    "\n",
    "* Schroff et al. 2015, FaceNet: A unified embedding for face recognition and clustering \n",
    "* One way to learn parameters to get a good encoding for faces is to define and apply gradient descent on triplet loss function\n",
    "* Suppose $A$ = anchor image, $P$ = positive (same person), $N$ = negative (different person)\n",
    "    * Want $\\Vert f(A) - f(P)\\Vert^2 \\leq \\Vert f(A) - f(N)\\Vert^2$ \n",
    "        * Trivial solution workaround since $\\Vert f(A) - f(P)\\Vert^2 - \\Vert f(A) - f(N)\\Vert^2 \\leq 0 $ you can just make everything zero, so we add a margin $\\alpha$ such that:\n",
    "           *  $\\Vert f(A) - f(P)\\Vert^2 - \\Vert f(A) - f(N)\\Vert^2 + \\alpha \\leq 0 $\n",
    "        * Thus, we have $\\Vert f(A) - f(P)\\Vert^2 + \\alpha \\leq \\Vert f(A) - f(N)\\Vert^2$\n",
    "        \n",
    "* Triplet loss function: Given 3 images $A, P, N$:\n",
    "    * Define $ L(A,P,N) = \\max(\\Vert f(A) - f(P)\\Vert^2 - \\Vert f(A) - f(N)\\Vert^2 + \\alpha, 0)$\n",
    "    * Thus, overall cost is $J = \\sum\\limits_{i=1}^{m}{L(A^{(i)},P^{(i)},N^{(i)})}$\n",
    "    * Training set can be: 10k pictures of 1k people with some pairs $A$ and $P$ of the same person \n",
    "        * During training, if $A,P,N$ are chosen randomly, then  $d(A,P) + \\alpha \\leq d(A,N)$ is easily satisfied\n",
    "        * So, choose triplets that are \"hard\" to train on s.t.:\n",
    "            * $d(A,P) \\approx d(A,N)$ so that the model has to train \"extra hard\" so that there is at least a margin $\\alpha$ between a positive and negative differential \n",
    "        * use gradient descent to minimize the cost as per usual \n",
    "* some companies used 100millions of images, just get the parameters they trained \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Verification and Binary Classification\n",
    "\n",
    "* Have a siamese NN and have two encodings $f(x^{(i)})$ feed into a logistic regression unit to make a prediction $\\hat{y}$ where $\\hat{y} = 1$ if they are the same person and $0$ if they are not.\n",
    "    * Alternative to the triplet loss \n",
    "    * This makes face recognition into a binary classificaion problem!\n",
    "    * Let's formulate $\\hat{y}$ as follows. Say you have an encoding that has $h$ features and $i$ and $j$ are 2 inputs (faces) : Then \n",
    "        * $\\hat{y} = \\sigma(\\sum\\limits_{k=1}^{h} w_i \\vert f(x^{(i)})_k - f(x^{(j)})_k \\vert + b) $\n",
    "        * Chi-square formula   \n",
    "    * precompute faces in a database so you only have to compute new images \n",
    "    * use different pairs to train the Siamese NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Style Transfer \n",
    "\n",
    "#### What is neural style transfer?\n",
    "\n",
    "* Given a content image $C$ and a style image $S$, output a generated image $G$ that is in the style of $S$ and has the contents of $C$\n",
    "    * Need to look at various features in the ConvNet, both shallow and deep for a better intuition \n",
    "    \n",
    "#### What are deep ConvNets learning? \n",
    "* Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks \n",
    "* Pick a unit in layer 1. Find 9 image patches that maximize the unit's activation. Repeat for other units. \n",
    "    * Figure out what a NN is actually learning in this first layer \n",
    "    * Do this also to later layers to visualize what's happening\n",
    "    \n",
    "#### Cost Function \n",
    "\n",
    "* Gatys et al., 2015. A neural algorithm of artistic style. \n",
    "* Remember $C,S,G$\n",
    "* We create a cost function $J(G)$ to measure how good a generated image is.\n",
    "    * $J(G) = \\alpha J_{\\text{content}}(C,G) + \\beta J_{\\text{style}}(S,G)$\n",
    "* Process to generate image $G$\n",
    "    * 1.) initialize $G$ pixels randomly: $G: 100 \\times 100 \\times 3$\n",
    "    * 2.) Use gradient descnt to minimize $J(G)$\n",
    "        * $G := G - \\dfrac{\\partial}{\\partial G}J(G)$\n",
    "        * i.e. update the pixels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Content Cost Function \n",
    "\n",
    "* Remember $J(G) = \\alpha J_{\\text{content}}(C,G) + \\beta J_{\\text{style}}(S,G)$\n",
    "    \n",
    "* Let's figure out the content cost function $J_{\\text{content}}(C,G)$\n",
    "* Use a hidden layer $l$ to compute content cost \n",
    "* Use pre-trained ConvNet \n",
    "* Let $a^{[l](C)}$ and $a^{[l](G)}$ be the activation of layer $l$ on the images \n",
    "    * if the $a$s are similar, both have similar content \n",
    "        * $J_{\\text{content}}(C,G) = \\dfrac{1}{2}\\Vert a^{[l](C)} - a^{[l](G)} \\Vert^2$ aka element-wise sum of squares of differences between the activations in layer $l$ of the content and generated image \n",
    "            * perform gradient descent on this content cost function \n",
    "            * incentivize the algorithm to find image $G$ so that the hidden layers are like the content image  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style Cost Function \n",
    "\n",
    "*  What **is** the style of an image? \n",
    "    * Defined as the **correlation between activations across channels** \n",
    "        * 'correlated' activations across different channels - if some part of an image has some characteristic, then the image will likely have another characteristic \n",
    "            * Ng's example uses vertical lines and the color orange\n",
    "        * 'uncorrelated' activations across different channels - if some part of an image has some characteristic, then the image will likely not have another characteristic  \n",
    "    * We can use the degree of correlations as a measure of the style\n",
    "    \n",
    "* Style matrix, basically we use multiplication to measure correlation between channels  \n",
    "    * Let $a^{[l]}_{i,j,k} =$ activation at $(i,j,k)$. $G^{[l]}$ is $n_c^{[l]} \\times n_c^{[l]}$\n",
    "        * We denote $G^{[l]}_{kk'}$ that will measure how correlated chanel $k$ is to channel $k'$ where $k = 1,...,n_c^{[l]}$\n",
    "        * style matrix:  $G^{[l]}_{kk'} = \\sum\\limits_{i=1}^{n_H^{[l]}}\\sum\\limits_{j=1}^{n_W^{[l]}} a^{[l]}_{i,j,k} a^{[l]}_{i,j,k'}$\n",
    "            * large if two channels are correlated, and small if two channels are not correlated \n",
    "    * We have a style matrix for both the style image and the generated image, s.t.:\n",
    "        * style image matrix: $G^{[l](S)}_{kk'} = \\sum\\limits_{i=1}^{n_H^{[l](S)}}\\sum\\limits_{j=1}^{n_W^{[l](S)}} a^{[l](S)}_{i,j,k} a^{[l](S)}_{i,j,k'}$\n",
    "        * generated image matrix: $G^{[l](G)}_{kk'} = \\sum\\limits_{i=1}^{n_H^{[l](G)}}\\sum\\limits_{j=1}^{n_W^{[l](G)}} a^{[l](G)}_{i,j,k} a^{[l](G)}_{i,j,k'}$ \n",
    "        * 'Gram matrix' in linear algebra \n",
    "* Thus, the style cost function for a layer $l$ is: \n",
    "    * $J^{[l]}_{\\text{style}}(S,G) = \\dfrac{1}{(2n_h^{[l]}n_W^{[l]}n_C^{[l]})^2}\\Vert G^{[l](S)} - G^{[l](G)}\\Vert^2_F = \\dfrac{1}{(2n_h^{[l]}n_W^{[l]}n_C^{[l]})^2} \\sum\\limits_k\\sum\\limits_{k'}(G^{[l](S)}_{kk'} - G^{[l](G)}_{kk'})^2$\n",
    "        * the normalization constant $\\dfrac{1}{(2n_h^{[l]}n_W^{[l]}n_C^{[l]})^2}$ doesn't matter that much because this cost is multiplied by $\\beta$, the hyperparameter we use during optimization \n",
    "\n",
    "* Thus, the style cost function over all the layers is:\n",
    "    * $J_{\\text{style}}(S,G) = \\sum\\limits_l \\lambda^{[l]}J_{\\text{style}}^{[l]}(S,G)$\n",
    "        * where $\\lambda$ is some hyperparameter to be weighted against each layer in the style cost function \n",
    "\n",
    "* THEREFORE, the overall cost function is: \n",
    "\n",
    "$$J(G) = \\alpha J_{\\text{content}}(C,G) + \\beta J_{\\text{style}}(S,G)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D and 3D Generalizations\n",
    "\n",
    "* 1D data, as you would expect\n",
    "    * $14 \\times 1 * 5 \\times 1 \\rightarrow 10 \\times 16$ where $*$ is a convolution operator \n",
    "    * alot of 1D data uses Recurrent Neural Networks, models designed specifically for sequence data \n",
    "*  3D, as you'd expect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Assignment: Art Generation with Neural Style Transfer\n",
    "\n",
    "#### Tensorflow notes\n",
    "\n",
    "* tf.eval() vs tf.run() \n",
    "    * tf.eval() fetches the value of only one tensor \n",
    "        * equivalent to calling tf.get_default_session().run(t) for some Tensor t\n",
    "    * tf.run() can fetch the values of multiple tensors in a single step \n",
    "* tf.reduce_sum() by default somes all elements in a tensor \n",
    "* though tf.set_random_seed(i) sets a random seed, the value of the tensor changes even though no operations have been done to it. Shouldn't the data be static at the time of evaluation?\n",
    "* use tf.square(x) to square x element-wise  \n",
    "\n",
    "#### Programming notes \n",
    "\n",
    "* remember:\n",
    "\n",
    "``\n",
    "hello = (1,2)\n",
    "print(type(hello)) # tuple in python\n",
    "``\n",
    "\n",
    "#### Lessons learned \n",
    "\n",
    "* Style matrix is Gram matrix! from linear algebra! ayyy :) \n",
    "    * (should probably take a linear algebra course and prob and stats course)\n",
    "* Perception change: \n",
    "    * 'One important part of the gram matrix is that the diagonal elements such as $G_{ii}$ also measures how active filter $i$ is. For example, suppose filter $i$ is detecting vertical textures in the image. Then $G_{ii}$ measures how common  vertical textures are in the image as a whole: If $G_{ii}$ is large, this means that the image has a lot of vertical texture.'\n",
    "\n",
    "* Content cost function used in assignment:\n",
    "$$J_{content}(C,G) =  \\frac{1}{4 \\times n_H \\times n_W \\times n_C}\\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2 $$\n",
    "\n",
    "* Style cost function used in assigment:\n",
    "$$J_{style}^{[l]}(S,G) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2 $$\n",
    "    * As such, the goal is to minimize the 'distance' of the style image from the generated image i.e. how far the generated image is from determining the measure of prevalence of a feature from how that feature's prevalence measure in the style image \n",
    "    \n",
    "* Why does the style matrix have dimensions $n_C \\times n_C$?\n",
    "    * style matrix is a measure of correlation between the activations of different channels for an image\n",
    "    \n",
    "    \n",
    "* What you should remember:\n",
    "    - The style of an image can be represented using the Gram matrix of a hidden layer's activations. However, we get even better results combining this representation from multiple different layers. This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.\n",
    "    - Minimizing the style cost will cause the image $G$ to follow the style of the image $S$. \n",
    "\n",
    "* Need to go back and look at optimizer functions and their benefits/differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
